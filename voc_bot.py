from google_play_scraper import reviews as gp_reviews
from app_store_web_scraper import AppStoreEntry
from datetime import datetime, timedelta
import pandas as pd
import openai
import time
import base64
import requests
import os
import json

#ë³€ìˆ˜
openai_api_key = os.environ.get("OPENAI_API_KEY")
client = openai.OpenAI(api_key=openai_api_key)
end_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
start_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
confluence_api_token = os.environ.get("CONFLUENCE_API_TOKEN")
confluence_api_user = os.environ.get("CONFLUENCE_API_USER")
space_key = 'CSO'
parent_page_id = '619544652'
title = "ì£¼ê°„ ì•± ë¦¬ë·° ë¶„ì„ ë¦¬í¬íŠ¸_" + datetime.now().strftime("%Y-%m-%d")

## ì´ì¦ì¶©ì „ì†Œ aos
google_app_id = 'com.locam.cashbeecharge'
result, _ = gp_reviews(
    google_app_id,
    lang='ko',
    country='kr',
    count=500
)
data_google = []
for r in result:
    data_google.append([
        'ìì‚¬',
        'ì´ì¦ì¶©ì „ì†Œ',       # ì•±ëª…ì¹­
        'android',       # OS
        r['at'].strftime('%Y-%m-%d'),  # ì‘ì„±ì¼
        r.get('reviewCreatedVersion', ''),
        r.get('title', ''),
        r['content'],
        r['score']
    ])
df_google_c = pd.DataFrame(data_google, columns=['êµ¬ë¶„', 'ì•±ëª…ì¹­', 'os', 'ì‘ì„±ì¼', 'ë²„ì „', 'ë¦¬ë·° ì œëª©', 'ë¦¬ë·° ë‚´ìš©', 'í‰ì '])

## ëª¨ë°”ì¼ì´ì¦
google_app_id = 'com.ebcard.cashbee3'
result, _ = gp_reviews(
    google_app_id,
    lang='ko',
    country='kr',
    count=500
)
data_google = []
for r in result:
    data_google.append([
        'ìì‚¬',
        'ëª¨ë°”ì¼ì´ì¦',       # ì•±ëª…ì¹­
        'android',       # OS
        r['at'].strftime('%Y-%m-%d'),  # ì‘ì„±ì¼
        r.get('reviewCreatedVersion', ''),
        r.get('title', ''),
        r['content'],
        r['score']
    ])
df_google_m = pd.DataFrame(data_google, columns=['êµ¬ë¶„', 'ì•±ëª…ì¹­', 'os', 'ì‘ì„±ì¼', 'ë²„ì „', 'ë¦¬ë·° ì œëª©', 'ë¦¬ë·° ë‚´ìš©', 'í‰ì '])

## ì´ì¦ì¶©ì „ì†Œ ios
app = AppStoreEntry(app_id=1519907149, country="kr")
reviews = list(app.reviews(limit=500))
data_apple = []
for r in reviews:
    review_date = r.date.strftime('%Y-%m-%d') if hasattr(r, "date") and r.date else ''
    data_apple.append([
        'ìì‚¬',
        'ì´ì¦ì¶©ì „ì†Œ',         # ì•±ëª…ì¹­
        'ios',              # OS
        review_date,        # ì‘ì„±ì¼
        '',                 # ë²„ì „(ì›¹ ìŠ¤í¬ë˜í•‘ì€ ì—†ìŒ)
        r.title or '',
        r.review or '',
        r.rating
    ])
df_apple = pd.DataFrame(data_apple, columns=['êµ¬ë¶„', 'ì•±ëª…ì¹­', 'os', 'ì‘ì„±ì¼', 'ë²„ì „', 'ë¦¬ë·° ì œëª©', 'ë¦¬ë·° ë‚´ìš©', 'í‰ì '])

## ëª¨ë°”ì¼í‹°ë¨¸ë‹ˆ
google_app_id = 'com.lgt.tmoney'
result, _ = gp_reviews(
    google_app_id,
    lang='ko',
    country='kr',
    count=500
)
data_google = []
for r in result:
    data_google.append([
        'ê²½ìŸì‚¬',
        'ëª¨ë°”ì¼í‹°ë¨¸ë‹ˆ',       # ì•±ëª…ì¹­
        'android',       # OS
        r['at'].strftime('%Y-%m-%d'),  # ì‘ì„±ì¼
        r.get('reviewCreatedVersion', ''),
        r.get('title', ''),
        r['content'],
        r['score']
    ])
df_google_t = pd.DataFrame(data_google, columns=['êµ¬ë¶„', 'ì•±ëª…ì¹­', 'os', 'ì‘ì„±ì¼', 'ë²„ì „', 'ë¦¬ë·° ì œëª©', 'ë¦¬ë·° ë‚´ìš©', 'í‰ì '])

## ëª¨ë°”ì¼í‹°ë¨¸ë‹ˆ ios
app = AppStoreEntry(app_id=1470361790, country="kr")
reviews = list(app.reviews(limit=500))
data_apple = []
for r in reviews:
    review_date = r.date.strftime('%Y-%m-%d') if hasattr(r, "date") and r.date else ''
    data_apple.append([
        'ê²½ìŸì‚¬',
        'ëª¨ë°”ì¼í‹°ë¨¸ë‹ˆ',         # ì•±ëª…ì¹­
        'ios',              # OS
        review_date,        # ì‘ì„±ì¼
        '',                 # ë²„ì „(ì›¹ ìŠ¤í¬ë˜í•‘ì€ ì—†ìŒ)
        r.title or '',
        r.review or '',
        r.rating
    ])
df_apple_t = pd.DataFrame(data_apple, columns=['êµ¬ë¶„', 'ì•±ëª…ì¹­', 'os', 'ì‘ì„±ì¼', 'ë²„ì „', 'ë¦¬ë·° ì œëª©', 'ë¦¬ë·° ë‚´ìš©', 'í‰ì '])

#í•©ì¹˜ê¸°
df_all = pd.concat([df_google_m, df_google_c, df_apple, df_google_t, df_apple_t], ignore_index=True)

#ê¸°ê°„ í•„í„°
start_date_dt = pd.to_datetime(start_date)
end_date_dt = pd.to_datetime(end_date)
df_all['ì‘ì„±ì¼'] = pd.to_datetime(df_all['ì‘ì„±ì¼'], errors='coerce')
df_all = df_all[(df_all['ì‘ì„±ì¼'] >= start_date_dt) & (df_all['ì‘ì„±ì¼'] <= end_date_dt)]

# ë¦¬ë·° í•©ì¹˜ê¸° (ì œëª©,ë‚´ìš©)
df_all['ë¦¬ë·° í…ìŠ¤íŠ¸'] = df_all['ë¦¬ë·° ì œëª©'].fillna('') + ' ' + df_all['ë¦¬ë·° ë‚´ìš©'].fillna('')
df_all = df_all.drop(['ë¦¬ë·° ì œëª©', 'ë¦¬ë·° ë‚´ìš©'], axis=1)

# ë°˜ì‘(ê¸ì •/ë¶€ì •) ì¹¼ëŸ¼ ì¶”ê°€
df_all['ë°˜ì‘'] = df_all['í‰ì '].apply(lambda x: 'ê¸ì •' if x >= 3 else 'ë¶€ì •')
df_all['ì¹´í…Œê³ ë¦¬'] = ''

df_all = df_all[['êµ¬ë¶„', 'ì•±ëª…ì¹­', 'os', 'ì‘ì„±ì¼', 'ë²„ì „', 'ë¦¬ë·° í…ìŠ¤íŠ¸', 'í‰ì ', 'ë°˜ì‘', 'ì¹´í…Œê³ ë¦¬']]
df_filtered = df_all[df_all['í‰ì '] <= 2].copy()

categories = """
ì¹´í…Œê³ ë¦¬ëª… | í¬í•¨ ì‚¬ë¡€/í‚¤ì›Œë“œ
---------|---------------------------------------------------------------------
ì•± ì ‘ì† ì˜¤ë¥˜ | íŠ•ê¹€, ì ‘ì†ì¥ì• , ê°•ì œì¢…ë£Œ, ë©ˆì¶¤, ë¡œë”© ë¬´í•œ, ë¹ˆí™”ë©´ ë“±
ì¸ì‹/ë“±ë¡/íƒœê¹…/ë°œê¸‰ | ì¹´ë“œ ë“±ë¡/ì¡°íšŒ ì‹¤íŒ¨, ì¹´ë“œ ë¯¸ì¸ì‹, íƒœê·¸ ì‹¤íŒ¨, ë°œí–‰, ë°œê¸‰, ìŠ¹í•˜ì°¨ ì˜¤ë¥˜, NFC ë¬¸ì œ ë“±
ì¶©ì „/ì”ì•¡/ê²°ì œ/í™˜ë¶ˆ | ì¶©ì „ ì‹¤íŒ¨/ì§€ì—°, ì”ì•¡ ì˜¤ë¥˜, ê²°ì œ/í™˜ë¶ˆ ì‹¤íŒ¨Â·ì§€ì—°, ì´ì¤‘ ê²°ì œ, ì¶©ì „ ë¯¸ì™„ë£Œ, ìˆ˜ìˆ˜ë£Œ ë¶ˆë§Œ, í™˜ë¶ˆ ì§€ì—° ë“±
ê¸°ê¸° í˜¸í™˜/ì—…ë°ì´íŠ¸ | ë‹¨ë§ê¸° í˜¸í™˜ì„± ë¬¸ì œ, íŠ¹ì • ê¸°ê¸°ì—ì„œ ì˜¤ë¥˜, OS/ì•± ì—…ë°ì´íŠ¸ í›„ ë¬¸ì œ, ìœ ì‹¬ ë¬¸ì œ, ì•± ë²„ì „ ë¬¸ì œ, ì‹ ê·œ ì—…ë°ì´íŠ¸ ì ìš© í›„ ì˜¤ë¥˜ ë°œìƒ ë“±
ê³ ê°ì„¼í„°/ë¬¸ì˜/CS | ê³ ê°ì„¼í„° ì—°ê²° ì•ˆ ë¨, ë¬¸ì˜/ìƒë‹´ ì§€ì—°, ë‹µë³€ ì—†ìŒ, ë¶ˆì¹œì ˆ, ë¯¼ì› ì²˜ë¦¬ ë¯¸í¡, CSì„¼í„° ì „í™” ì•ˆ ë¨, ì˜¨ë¼ì¸ ë¬¸ì˜ ë‹µë³€ ì§€ì—°, ê³ ê° ì‘ëŒ€ ë¶ˆë§Œ ë“±
UI/UX/ì´ìš©ë¶ˆí¸/ì¸ì¦ | í™”ë©´ êµ¬ì„± ë¶ˆí¸, ë²„íŠ¼/ë©”ë‰´ ì°¾ê¸° ì–´ë ¤ì›€, ë””ìì¸ ë¶ˆë§Œ, ì¸ì¦ ì‹¤íŒ¨, ë³¸ì¸ ì¸ì¦ ë°˜ë³µ ìš”êµ¬, ì ˆì°¨ ê³¼ë‹¤ ë“±
ì•Œë¦¼/í‘¸ì‹œ/ê³µì§€ | ì•Œë¦¼ì´ ì˜¤ì§€ ì•ŠìŒ, í‘¸ì‹œ ë¯¸ìˆ˜ì‹ , ê³µì§€ì‚¬í•­ ë¯¸ë…¸ì¶œ, ì´ë²¤íŠ¸/ê³µì§€ ì•Œë¦¼ ì§€ì—°, ë¶ˆí•„ìš”í•œ ì•Œë¦¼, ì•Œë¦¼ ì„¤ì • ë¬¸ì œ ë“±
ê¸°íƒ€ | ìœ„ í•­ëª©ì— í¬í•¨ë˜ì§€ ì•ŠëŠ” ê¸°íƒ€ ë¬¸ì œ, ê¸°íƒ€ ê±´ì˜ì‚¬í•­, ë‹¨ìˆœ ë¶€ì •, ì•± ì „ë°˜ì— ëŒ€í•œ ì˜ê²¬ ë“±
"""

#ì§€í”¼í‹°ê°€ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜í•´ì„œ ë„£ê¸°
def classify_with_gpt(text):
    prompt = f"""
ì•„ë˜ëŠ” ì•± ë¦¬ë·°ì…ë‹ˆë‹¤.

ë¦¬ë·°: "{text}"

ì•„ë˜ í‘œì˜ ì¹´í…Œê³ ë¦¬ì™€ ì˜ˆì‹œ í‚¤ì›Œë“œë¥¼ ì°¸ê³ í•˜ì—¬, ë°˜ë“œì‹œ ê°€ì¥ ê°€ê¹Œìš´ ì¹´í…Œê³ ë¦¬ëª… í•œ ê°œë§Œ ì„ íƒí•˜ì‹­ì‹œì˜¤.
ë°˜ë“œì‹œ ë‹¤ìŒ ê·œì¹™ì„ ì§€í‚¤ì„¸ìš”:
- ì•„ë˜ 8ê°œ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë§Œ ê³¨ë¼ì„œ, ì¹´í…Œê³ ë¦¬ëª…ë§Œ ì •í™•í•˜ê²Œ ì¶œë ¥í•˜ì„¸ìš”.
- ê° ì¹´í…Œê³ ë¦¬ëª… ì˜†ì˜ ì˜ˆì‹œ í‚¤ì›Œë“œë¥¼ ì°¸ê³ í•´ ê·¼ì ‘í•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”.
- ë‹¨, ì˜ë¯¸ê°€ ëª…í™•í•˜ì§€ ì•Šê±°ë‚˜, ë‚´ìš©ì´ ì—†ëŠ” ë‹¨ìˆœ ìš”ì²­Â·ìš•ì„¤Â·ë¶ˆí‰ì€ 'ê¸°íƒ€'ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
- ì˜¤ì§ ì¹´í…Œê³ ë¦¬ëª…ë§Œ ì¶œë ¥í•˜ì„¸ìš”. (ì„¤ëª…, ë²ˆí˜¸ ë“± ê¸ˆì§€)

{categories}

"""
    response = client.chat.completions.create(
        model="gpt-4.1",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=20,
        temperature=0
    )
    return response.choices[0].message.content.strip()

#ë¶„ë¥˜ ì ìš©
for idx, row in df_filtered.iterrows():
    review_text = row['ë¦¬ë·° í…ìŠ¤íŠ¸'][:100]
    try:
        category = classify_with_gpt(review_text)
    except Exception as e:
        print(f"ì˜ˆì™¸ ë°œìƒ: {e}")
        category = "ê¸°íƒ€"
    df_filtered.at[idx, 'ì¹´í…Œê³ ë¦¬'] = category
    time.sleep(1.2)


#ì›ë³¸ df_allì— ê²°ê³¼ ë°˜ì˜
df_all.loc[df_filtered.index, 'ì¹´í…Œê³ ë¦¬'] = df_filtered['ì¹´í…Œê³ ë¦¬']

summary = []

for company in ['ìì‚¬', 'ê²½ìŸì‚¬']:
    df_sub = df_all[df_all['êµ¬ë¶„'] == company]
    review_count = len(df_sub)
    positive = (df_sub['ë°˜ì‘'] == 'ê¸ì •').sum()
    negative = (df_sub['ë°˜ì‘'] == 'ë¶€ì •').sum()
    positive_ratio = round(negative / review_count * 100, 1) if review_count > 0 else 0
    avg_score = round(df_sub['í‰ì '].mean(), 2) if review_count > 0 else 0
    summary.append([company, review_count, positive, negative, positive_ratio, avg_score])

# 2. ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
summary_df = pd.DataFrame(summary, columns=['êµ¬ë¶„', 'ë¦¬ë·°ìˆ˜', 'ê¸ì •', 'ë¶€ì •', 'ë¶€ì •ë¹„ìœ¨', 'í‰ê· í‰ì '])

# [1] ë°ì´í„° ì €ì¥
df_all.to_csv('df_all.csv', index=False, encoding='utf-8-sig')
csv_file_path = 'df_all.csv'

# [2] ìì‚¬/ê²½ìŸì‚¬ë³„ ë¦¬ë·°Â·í‰ì  ìš”ì•½í‘œ
summary = []
for company in ['ìì‚¬', 'ê²½ìŸì‚¬']:
    df_sub = df_all[df_all['êµ¬ë¶„'] == company]
    review_count = len(df_sub)
    positive = (df_sub['ë°˜ì‘'] == 'ê¸ì •').sum()
    negative = (df_sub['ë°˜ì‘'] == 'ë¶€ì •').sum()
    negative_ratio = round(negative / review_count * 100, 1) if review_count > 0 else 0
    avg_score = round(df_sub['í‰ì '].mean(), 2) if review_count > 0 else 0
    summary.append([company, review_count, positive, negative, negative_ratio, avg_score])

summary_df = pd.DataFrame(summary, columns=['êµ¬ë¶„', 'ë¦¬ë·°ìˆ˜', 'ê¸ì •', 'ë¶€ì •', 'ë¶€ì •ë¹„ìœ¨', 'í‰ê· í‰ì '])

# [3] ì¹´í…Œê³ ë¦¬ë³„ ë¶€ì • ë¦¬ë·° ë¶„í¬í‘œ
df_neg = df_all[df_all['ë°˜ì‘'] == 'ë¶€ì •']
category_table = df_neg.pivot_table(
    index='ì¹´í…Œê³ ë¦¬',
    columns='êµ¬ë¶„',
    values='ë¦¬ë·° í…ìŠ¤íŠ¸',
    aggfunc='count',
    fill_value=0
).reset_index()
category_table.columns.name = None
total_ja = category_table['ìì‚¬'].sum()
total_comp = category_table['ê²½ìŸì‚¬'].sum()
category_table['ë¹„ì¤‘(ìì‚¬)'] = (category_table['ìì‚¬'] / total_ja * 100).round(2)
category_table['ë¹„ì¤‘(ê²½ìŸì‚¬)'] = (category_table['ê²½ìŸì‚¬'] / total_comp * 100).round(2)
category_table = category_table[['ì¹´í…Œê³ ë¦¬', 'ìì‚¬', 'ë¹„ì¤‘(ìì‚¬)', 'ê²½ìŸì‚¬', 'ë¹„ì¤‘(ê²½ìŸì‚¬)']]
category_order = [
    "ì•± ì ‘ì† ì˜¤ë¥˜", "ì¸ì‹/ë“±ë¡/íƒœê¹…/ë°œê¸‰", "ì¶©ì „/ì”ì•¡/ê²°ì œ/í™˜ë¶ˆ",
    "ê¸°ê¸° í˜¸í™˜/ì—…ë°ì´íŠ¸", "ê³ ê°ì„¼í„°/ë¬¸ì˜/CS",
    "UI/UX/ì´ìš©ë¶ˆí¸/ì¸ì¦", "ì•Œë¦¼/í‘¸ì‹œ/ê³µì§€", "ê¸°íƒ€"
]
category_table['ì¹´í…Œê³ ë¦¬'] = pd.Categorical(category_table['ì¹´í…Œê³ ë¦¬'], categories=category_order, ordered=True)
category_table = category_table.sort_values('ì¹´í…Œê³ ë¦¬')


# ê¸ì •/ë¶€ì • êµ¬ë¶„ í¬í•¨ ì „ì²´ ì¹´í…Œê³ ë¦¬ ì§‘ê³„
rows = []

for reaction in ['ê¸ì •', 'ë¶€ì •']:
    if reaction == 'ê¸ì •':
        # ê¸ì •ì€ ì¹´í…Œê³ ë¦¬ ì—†ì´ í•œì¤„
        for os_name in ['android', 'ios']:
            cnt = len(df_all[(df_all['êµ¬ë¶„'] == 'ìì‚¬') & (df_all['os'] == os_name) & (df_all['ë°˜ì‘'] == reaction)])
            rows.append([reaction, '', cnt if os_name == 'android' else '', '' if os_name == 'android' else cnt])
        continue
    # ë¶€ì •: ì¹´í…Œê³ ë¦¬ë³„ ì§‘ê³„
    for cat in category_order:
        row = [reaction, cat]
        for os_name in ['android', 'ios']:
            mask = (
                (df_all['êµ¬ë¶„'] == 'ìì‚¬') &
                (df_all['os'] == os_name) &
                (df_all['ë°˜ì‘'] == reaction) &
                (df_all['ì¹´í…Œê³ ë¦¬'] == cat)
            )
            cnt = df_all[mask].shape[0]
            row.append(cnt)
        rows.append(row)

# ë°ì´í„°í”„ë ˆì„ ìƒì„±
os_cat_df = pd.DataFrame(
    rows,
    columns=[
        'ë°˜ì‘', 'ì¹´í…Œê³ ë¦¬',
        'ë¦¬ë·° ê±´ìˆ˜(android)', 'ë¦¬ë·° ê±´ìˆ˜(ios)'
    ]
)

# OSë³„ ë¶€ì • í•©ê³„ ê³„ì‚°
for os_name in ['android', 'ios']:
    total_neg = os_cat_df.loc[
        (os_cat_df['ë°˜ì‘'] == 'ë¶€ì •'), f'ë¦¬ë·° ê±´ìˆ˜({os_name})'
    ].sum()
    os_cat_df[f'ë¹„ì¤‘ ({os_name})'] = os_cat_df.apply(
        lambda row: round((row[f'ë¦¬ë·° ê±´ìˆ˜({os_name})'] / total_neg * 100), 2) if row['ë°˜ì‘'] == 'ë¶€ì •' and total_neg > 0 else '',
        axis=1
    )

# ì¹¼ëŸ¼ ìˆœì„œ ì¬ë°°ì¹˜
os_cat_df = os_cat_df[
    ['ë°˜ì‘', 'ì¹´í…Œê³ ë¦¬',
     'ë¦¬ë·° ê±´ìˆ˜(android)', 'ë¹„ì¤‘ (android)',
     'ë¦¬ë·° ê±´ìˆ˜(ios)', 'ë¹„ì¤‘ (ios)']
]

# HTML ë³€í™˜
os_cat_html = os_cat_df.to_html(index=False, border=1)


# [4] HTML ë³€í™˜ (ì»¨í”Œë£¨ì–¸ìŠ¤ ë³¸ë¬¸)
summary_html = summary_df.to_html(index=False, border=1)
category_html = category_table.to_html(index=False, border=1)

# [5] ì£¼ìš” ì¸ì‚¬ì´íŠ¸(gpt_anal) ìƒì„±
ja_texts = "\n".join(
    df_filtered[df_filtered['êµ¬ë¶„'] == 'ìì‚¬'].sort_values('ì‘ì„±ì¼', ascending=False)['ë¦¬ë·° í…ìŠ¤íŠ¸']
)
comp_texts = "\n".join(
    df_filtered[df_filtered['êµ¬ë¶„'] == 'ê²½ìŸì‚¬'].sort_values('ì‘ì„±ì¼', ascending=False)['ë¦¬ë·° í…ìŠ¤íŠ¸']
)


gpt_prompt = f"""ì•„ë˜ëŠ” ì•± ë¶€ì • ë¦¬ë·° ë°ì´í„° ì…ë‹ˆë‹¤.

[ìì‚¬ ë¶€ì • ë¦¬ë·°]
{ja_texts}

[ê²½ìŸì‚¬ ë¶€ì • ë¦¬ë·°]
{comp_texts}

ë¦¬ë·° ë°ì´í„°ë¥¼ ì°¸ê³ í•´ ì•„ë˜ ì–‘ì‹ì— ë§ê²Œ ìš”ì•½í•´ ì£¼ì„¸ìš”.

[ìì‚¬ ë¶€ì • ì´ìŠˆ]
- ì‘ì„±í•´ì£¼ì„¸ìš”
- ì‘ì„±í•´ì£¼ì„¸ìš”
- ì‘ì„±í•´ì£¼ì„¸ìš” 

[ê²½ìŸì‚¬ ë¶€ì • ì´ìŠˆ]
- ì‘ì„±í•´ì£¼ì„¸ìš” 
- ì‘ì„±í•´ì£¼ì„¸ìš”
- ì‘ì„±í•´ì£¼ì„¸ìš”


ê³µí†µê·œì¹™:
- let's think step by step and work through this carefully
- ìì‚¬/íƒ€ì‚¬ ë¶€ì • ì´ìŠˆ : "ìì‚¬ ë¶€ì • ë¦¬ë·°", "ê²½ìŸì‚¬ ë¶€ì • ë¦¬ë·°" ê° ë¦¬ë·°ë¥¼ ì½ê³  ìì‚¬,ê²½ìŸì‚¬ë³„ ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ ë¬¸ì œë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤ ì´ëŠ” ì–´ë–¤ ì´ìŠˆê°€ ê°€ì¥ ë§ì€ì§€ í™•ì¸í•˜ê¸° ìœ„í•¨ìœ¼ë¡œ ì •í™•í•´ì•¼ í•©ë‹ˆë‹¤.
- ë¶€ì • ì´ìŠˆëŠ” êµ¬ì²´ì ì¸ ë¬¸ì œ ìœ í˜•(ì˜ˆ: ì¶©ì „ ì˜¤ë¥˜, ì¹´ë“œ ë“±ë¡ ì‹¤íŒ¨, ì•± íŠ•ê¹€ ë“±) ìœ„ì£¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
- ì–´ë–¤ ë¶€ì •ì ì¸ ê²½í—˜ì„ í†µí•´ ê³ ê°ì´ ë¦¬ë·°ë¥¼ ë‚¨ê¸°ê²Œê¹Œì§€ ë˜ëŠ”ì§€, ì–´ë–¤ë¶€ë¶„ì´ ì¹˜ëª…ì  ì´ì˜€ëŠ”ì§€ íŒŒì•…ì´ ë˜ì—ˆìœ¼ë©´ í•©ë‹ˆë‹¤.
- ê° í•­ëª©ë³„ "ì‘ì„±í•´ì£¼ì„¸ìš”"ëŠ” 3ê°œì´ì§€ë§Œ ìµœì†Œ 3ì¤„ë§Œ ë§Œì¡±í•˜ë©´ë©ë‹ˆë‹¤, ìµœëŒ€ 6ì¤„ê¹Œì§€ í—ˆìš©í•©ë‹ˆë‹¤
- ê²°ê³¼ë¬¼ì€ ì»¨í”Œë£¨ì–¸ìŠ¤ APIë¥¼ í†µí•´ body_htmlë¡œ ì „ë‹¬ë˜ë©° ì§€ì •í•´ì¤€ ì œëª©,ì¤„ë°”ê¿ˆì€ ë³€ê²½í•˜ì§€ ë§ì•„ì£¼ì„¸ìš”
"""

# GPT API í˜¸ì¶œ
response = client.chat.completions.create(
    model="gpt-4.1",
    messages=[
        {"role": "system", "content": "ë„ˆëŠ” ë°ì´í„° ë¶„ì„ê°€ì•¼."},
        {"role": "user", "content": gpt_prompt}
    ],
    max_tokens=512,
    temperature=0.5,
)
gpt_anal_text = response.choices[0].message.content.strip()
gpt_anal = f"<pre>{gpt_anal_text}</pre>"

# [6] body_htmlì— gpt_anal ì‚½ì…
body_html = f"""
<br>
<h2>ğŸ“ ì°¸ê³  ì‚¬í•­</h2>
<ul>
  <li>ë³¸ ë³´ê³ ì„œëŠ” ë§¤ì£¼ ì›”ìš”ì¼ ì˜¤ì „ 7ì‹œì— ìë™ ë°œì†¡ë©ë‹ˆë‹¤.</li>
  <li>ì§€ë‚œì£¼(ì›”~ì¼) ì‘ì„±ëœ ì•± ë¦¬ë·°ë“¤ì„ ìˆ˜ì§‘í•˜ì—¬ ë¶„ì„ ë° ìš”ì•½í•œ ìë£Œì…ë‹ˆë‹¤.</li>
  <li>í‰ì  3ì  ì´ìƒì€ 'ê¸ì •'ìœ¼ë¡œ ë¶„ë¥˜, 2ì  ì´í•˜ëŠ” 'ë¶€ì •'ìœ¼ë¡œ ë¶„ë¥˜í•˜ì˜€ìŠµë‹ˆë‹¤.</li>
</ul>
<br>
<h2>ğŸ“Š 1. ë¦¬ë·°/í‰ì  ìš”ì•½í‘œ</h2>
{summary_html}
<br>
<h2>ğŸ“ 2. ë¶€ì • ë¦¬ë·° ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬</h2>
{category_html}
<br>
<h2>ğŸ§© 3. ìì‚¬ì•± OSë³„ ë¦¬ë·° ë¶„í¬</h2>
{os_cat_html}
<br>
<h2>ğŸ’¡ 4. ì£¼ìš” ì¸ì‚¬ì´íŠ¸</h2>
{gpt_anal}
<br>
<h2>ğŸ“ [Raw Data] ë¦¬ë·° ë°ì´í„° ë‹¤ìš´ë¡œë“œ</h2>
"""

# [7] Confluence í˜ì´ì§€ ìƒì„±
confluence_domain = 'myezl.atlassian.net'

headers = {
    'Authorization': 'Basic ' + base64.b64encode(f'{confluence_api_user}:{confluence_api_token}'.encode()).decode(),
    'Content-Type': 'application/json'
}
base_url = f'https://{confluence_domain}/wiki/rest/api/content/'

page_data = {
    "type": "page",
    "title": title,
    "ancestors": [{"id": parent_page_id}],
    "space": {"key": space_key},
    "body": {
        "storage": {
            "value": body_html,
            "representation": "storage"
        }
    }
}
response = requests.post(base_url, headers=headers, json=page_data)
assert response.status_code in [200, 201], "í˜ì´ì§€ ìƒì„± ì‹¤íŒ¨: " + response.text
new_page_id = response.json()['id']
print("í˜ì´ì§€ ìƒì„± ì™„ë£Œ:", new_page_id)

# [8] ì²¨ë¶€íŒŒì¼ ì—…ë¡œë“œ
time.sleep(2)
attach_url = f"https://{confluence_domain}/wiki/rest/api/content/{new_page_id}/child/attachment"
attach_headers = {
    "Authorization": "Basic " + base64.b64encode(f"{confluence_api_user}:{confluence_api_token}".encode()).decode(),
    "X-Atlassian-Token": "no-check",
}
with open(csv_file_path, "rb") as f:
    files = {"file": (os.path.basename(csv_file_path), f, "text/csv")}
    attach_resp = requests.post(attach_url, headers=attach_headers, files=files)
attach_resp.raise_for_status()

# ì—…ë¡œë“œëœ ì‹¤ì œ íŒŒì¼ëª…Â·download ë§í¬ ì¶”ì¶œ
result_info   = attach_resp.json()["results"][0]
filename      = result_info["title"]                       # df_all.csv ë˜ëŠ” df_all.csv (1)
download_path = result_info["_links"]["download"]          # /download/attachments/...

print("ì²¨ë¶€íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ:", filename)

# [9] ë³¸ë¬¸ì— ì²¨ë¶€íŒŒì¼ ë§í¬ PATCH (ri:attachment ë§¤í¬ë¡œ)
# í˜„ì¬ í˜ì´ì§€ ë²„ì „ ì¡°íšŒ
ver_resp = requests.get(f"{base_url}{new_page_id}?expand=version", headers=headers)
cur_ver  = ver_resp.json()["version"]["number"]

attachment_macro = f'<p><ac:link><ri:attachment ri:filename="{filename}" /></ac:link></p>'
new_body_html    = body_html + attachment_macro

patch_data = {
    "version": {"number": cur_ver + 1, "minorEdit": True},
    "type": "page",
    "title": title,
    "body": {
        "storage": {
            "value": new_body_html,
            "representation": "storage"
        }
    }
}

patch_resp = requests.put(f"{base_url}{new_page_id}", headers=headers, json=patch_data)
patch_resp.raise_for_status()
print("ë³¸ë¬¸ì— ì²¨ë¶€íŒŒì¼ ë§í¬ ì¶”ê°€ ì™„ë£Œ:", patch_resp.status_code)
