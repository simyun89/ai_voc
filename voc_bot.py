from google_play_scraper import reviews as gp_reviews
from app_store_web_scraper import AppStoreEntry
from datetime import datetime, timedelta
import pandas as pd
import openai
import time
import base64
import requests
import os
import json

#ë³€ìˆ˜
openai_api_key = os.environ.get("OPENAI_API_KEY")
client = openai.OpenAI(api_key=openai_api_key)
end_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
start_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
confluence_api_token = os.environ.get("CONFLUENCE_API_TOKEN")
confluence_api_user = os.environ.get("CONFLUENCE_API_USER")
space_key = 'CSO'
parent_page_id = '619544652'
title = "ìë™ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒ˜í”Œ_" + datetime.now().strftime("%Y-%m-%d %H:%M:%S")

## ì´ì¦ì¶©ì „ì†Œ aos
google_app_id = 'com.locam.cashbeecharge'
result, _ = gp_reviews(
    google_app_id,
    lang='ko',
    country='kr',
    count=500
)
data_google = []
for r in result:
    data_google.append([
        'ìì‚¬',
        'ì´ì¦ì¶©ì „ì†Œ',       # ì•±ëª…ì¹­
        'android',       # OS
        r['at'].strftime('%Y-%m-%d'),  # ì‘ì„±ì¼
        r.get('reviewCreatedVersion', ''),
        r.get('title', ''),
        r['content'],
        r['score']
    ])
df_google_c = pd.DataFrame(data_google, columns=['êµ¬ë¶„', 'ì•±ëª…ì¹­', 'os', 'ì‘ì„±ì¼', 'ë²„ì „', 'ë¦¬ë·° ì œëª©', 'ë¦¬ë·° ë‚´ìš©', 'í‰ì '])

## ëª¨ë°”ì¼ì´ì¦
google_app_id = 'com.ebcard.cashbee3'
result, _ = gp_reviews(
    google_app_id,
    lang='ko',
    country='kr',
    count=500
)
data_google = []
for r in result:
    data_google.append([
        'ìì‚¬',
        'ëª¨ë°”ì¼ì´ì¦',       # ì•±ëª…ì¹­
        'android',       # OS
        r['at'].strftime('%Y-%m-%d'),  # ì‘ì„±ì¼
        r.get('reviewCreatedVersion', ''),
        r.get('title', ''),
        r['content'],
        r['score']
    ])
df_google_m = pd.DataFrame(data_google, columns=['êµ¬ë¶„', 'ì•±ëª…ì¹­', 'os', 'ì‘ì„±ì¼', 'ë²„ì „', 'ë¦¬ë·° ì œëª©', 'ë¦¬ë·° ë‚´ìš©', 'í‰ì '])

## ì´ì¦ì¶©ì „ì†Œ ios
app = AppStoreEntry(app_id=1470361790, country="kr")
reviews = list(app.reviews(limit=500))
data_apple = []
for r in reviews:
    review_date = r.date.strftime('%Y-%m-%d') if hasattr(r, "date") and r.date else ''
    data_apple.append([
        'ìì‚¬',
        'ì´ì¦ì¶©ì „ì†Œ',         # ì•±ëª…ì¹­
        'ios',              # OS
        review_date,        # ì‘ì„±ì¼
        '',                 # ë²„ì „(ì›¹ ìŠ¤í¬ë˜í•‘ì€ ì—†ìŒ)
        r.title or '',
        r.review or '',
        r.rating
    ])
df_apple = pd.DataFrame(data_apple, columns=['êµ¬ë¶„', 'ì•±ëª…ì¹­', 'os', 'ì‘ì„±ì¼', 'ë²„ì „', 'ë¦¬ë·° ì œëª©', 'ë¦¬ë·° ë‚´ìš©', 'í‰ì '])

## ëª¨ë°”ì¼í‹°ë¨¸ë‹ˆ
google_app_id = 'com.lgt.tmoney'
result, _ = gp_reviews(
    google_app_id,
    lang='ko',
    country='kr',
    count=500
)
data_google = []
for r in result:
    data_google.append([
        'ê²½ìŸì‚¬',
        'ëª¨ë°”ì¼í‹°ë¨¸ë‹ˆ',       # ì•±ëª…ì¹­
        'android',       # OS
        r['at'].strftime('%Y-%m-%d'),  # ì‘ì„±ì¼
        r.get('reviewCreatedVersion', ''),
        r.get('title', ''),
        r['content'],
        r['score']
    ])
df_google_t = pd.DataFrame(data_google, columns=['êµ¬ë¶„', 'ì•±ëª…ì¹­', 'os', 'ì‘ì„±ì¼', 'ë²„ì „', 'ë¦¬ë·° ì œëª©', 'ë¦¬ë·° ë‚´ìš©', 'í‰ì '])

## ëª¨ë°”ì¼í‹°ë¨¸ë‹ˆ ios
app = AppStoreEntry(app_id=1519907149, country="kr")
reviews = list(app.reviews(limit=500))
data_apple = []
for r in reviews:
    review_date = r.date.strftime('%Y-%m-%d') if hasattr(r, "date") and r.date else ''
    data_apple.append([
        'ê²½ìŸì‚¬',
        'ëª¨ë°”ì¼í‹°ë¨¸ë‹ˆ',         # ì•±ëª…ì¹­
        'ios',              # OS
        review_date,        # ì‘ì„±ì¼
        '',                 # ë²„ì „(ì›¹ ìŠ¤í¬ë˜í•‘ì€ ì—†ìŒ)
        r.title or '',
        r.review or '',
        r.rating
    ])
df_apple_t = pd.DataFrame(data_apple, columns=['êµ¬ë¶„', 'ì•±ëª…ì¹­', 'os', 'ì‘ì„±ì¼', 'ë²„ì „', 'ë¦¬ë·° ì œëª©', 'ë¦¬ë·° ë‚´ìš©', 'í‰ì '])

#í•©ì¹˜ê¸°
df_all = pd.concat([df_google_m, df_google_c, df_apple, df_google_t, df_apple_t], ignore_index=True)

#ê¸°ê°„ í•„í„°
start_date_dt = pd.to_datetime(start_date)
end_date_dt = pd.to_datetime(end_date)
df_all['ì‘ì„±ì¼'] = pd.to_datetime(df_all['ì‘ì„±ì¼'], errors='coerce')
df_all = df_all[(df_all['ì‘ì„±ì¼'] >= start_date_dt) & (df_all['ì‘ì„±ì¼'] <= end_date_dt)]

# ë¦¬ë·° í•©ì¹˜ê¸° (ì œëª©,ë‚´ìš©)
df_all['ë¦¬ë·° í…ìŠ¤íŠ¸'] = df_all['ë¦¬ë·° ì œëª©'].fillna('') + ' ' + df_all['ë¦¬ë·° ë‚´ìš©'].fillna('')
df_all = df_all.drop(['ë¦¬ë·° ì œëª©', 'ë¦¬ë·° ë‚´ìš©'], axis=1)

# ë°˜ì‘(ê¸ì •/ë¶€ì •) ì¹¼ëŸ¼ ì¶”ê°€
df_all['ë°˜ì‘'] = df_all['í‰ì '].apply(lambda x: 'ê¸ì •' if x >= 3 else 'ë¶€ì •')
df_all['ì¹´í…Œê³ ë¦¬'] = ''

df_all = df_all[['êµ¬ë¶„', 'ì•±ëª…ì¹­', 'os', 'ì‘ì„±ì¼', 'ë²„ì „', 'ë¦¬ë·° í…ìŠ¤íŠ¸', 'í‰ì ', 'ë°˜ì‘', 'ì¹´í…Œê³ ë¦¬']]
df_filtered = df_all[df_all['í‰ì '] <= 2].copy()

categories = """
ì¹´í…Œê³ ë¦¬ëª… | í¬í•¨ ì‚¬ë¡€/í‚¤ì›Œë“œ
---------|---------------------------------------------------------------------
ì•± ì ‘ì† ì˜¤ë¥˜ | íŠ•ê¹€, ì ‘ì†ì¥ì• , ê°•ì œì¢…ë£Œ, ë©ˆì¶¤, ë¡œë”© ë¬´í•œ, ë¹ˆí™”ë©´ ë“±
ì¸ì‹/ë“±ë¡/íƒœê¹…/ë°œê¸‰ | ì¹´ë“œ ë“±ë¡/ì¡°íšŒ ì‹¤íŒ¨, ì¹´ë“œ ë¯¸ì¸ì‹, íƒœê·¸ ì‹¤íŒ¨, ë°œí–‰, ë°œê¸‰, ìŠ¹í•˜ì°¨ ì˜¤ë¥˜, NFC ë¬¸ì œ ë“±
ì¶©ì „/ì”ì•¡/ê²°ì œ/í™˜ë¶ˆ | ì¶©ì „ ì‹¤íŒ¨/ì§€ì—°, ì”ì•¡ ì˜¤ë¥˜, ê²°ì œ/í™˜ë¶ˆ ì‹¤íŒ¨Â·ì§€ì—°, ì´ì¤‘ ê²°ì œ, ì¶©ì „ ë¯¸ì™„ë£Œ, ìˆ˜ìˆ˜ë£Œ ë¶ˆë§Œ, í™˜ë¶ˆ ì§€ì—° ë“±
ê¸°ê¸° í˜¸í™˜/ì—…ë°ì´íŠ¸ | ë‹¨ë§ê¸° í˜¸í™˜ì„± ë¬¸ì œ, íŠ¹ì • ê¸°ê¸°ì—ì„œ ì˜¤ë¥˜, OS/ì•± ì—…ë°ì´íŠ¸ í›„ ë¬¸ì œ, ìœ ì‹¬ ë¬¸ì œ, ì•± ë²„ì „ ë¬¸ì œ, ì‹ ê·œ ì—…ë°ì´íŠ¸ ì ìš© í›„ ì˜¤ë¥˜ ë°œìƒ ë“±
ê³ ê°ì„¼í„°/ë¬¸ì˜/CS | ê³ ê°ì„¼í„° ì—°ê²° ì•ˆ ë¨, ë¬¸ì˜/ìƒë‹´ ì§€ì—°, ë‹µë³€ ì—†ìŒ, ë¶ˆì¹œì ˆ, ë¯¼ì› ì²˜ë¦¬ ë¯¸í¡, CSì„¼í„° ì „í™” ì•ˆ ë¨, ì˜¨ë¼ì¸ ë¬¸ì˜ ë‹µë³€ ì§€ì—°, ê³ ê° ì‘ëŒ€ ë¶ˆë§Œ ë“±
UI/UX/ì´ìš©ë¶ˆí¸/ì¸ì¦ | í™”ë©´ êµ¬ì„± ë¶ˆí¸, ë²„íŠ¼/ë©”ë‰´ ì°¾ê¸° ì–´ë ¤ì›€, ë””ìì¸ ë¶ˆë§Œ, ì¸ì¦ ì‹¤íŒ¨, ë³¸ì¸ ì¸ì¦ ë°˜ë³µ ìš”êµ¬, ì ˆì°¨ ê³¼ë‹¤ ë“±
ì•Œë¦¼/í‘¸ì‹œ/ê³µì§€ | ì•Œë¦¼ì´ ì˜¤ì§€ ì•ŠìŒ, í‘¸ì‹œ ë¯¸ìˆ˜ì‹ , ê³µì§€ì‚¬í•­ ë¯¸ë…¸ì¶œ, ì´ë²¤íŠ¸/ê³µì§€ ì•Œë¦¼ ì§€ì—°, ë¶ˆí•„ìš”í•œ ì•Œë¦¼, ì•Œë¦¼ ì„¤ì • ë¬¸ì œ ë“±
ê¸°íƒ€ | ìœ„ í•­ëª©ì— í¬í•¨ë˜ì§€ ì•ŠëŠ” ê¸°íƒ€ ë¬¸ì œ, ê¸°íƒ€ ê±´ì˜ì‚¬í•­, ë‹¨ìˆœ ë¶€ì •, ì•± ì „ë°˜ì— ëŒ€í•œ ì˜ê²¬ ë“±
"""

#ì§€í”¼í‹°ê°€ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜í•´ì„œ ë„£ê¸°
def classify_with_gpt(text):
    prompt = f"""
ì•„ë˜ëŠ” ì•± ë¦¬ë·°ì…ë‹ˆë‹¤.

ë¦¬ë·°: "{text}"

ì•„ë˜ í‘œì˜ ì¹´í…Œê³ ë¦¬ì™€ ì˜ˆì‹œ í‚¤ì›Œë“œë¥¼ ì°¸ê³ í•˜ì—¬, ë°˜ë“œì‹œ ê°€ì¥ ê°€ê¹Œìš´ ì¹´í…Œê³ ë¦¬ëª… í•œ ê°œë§Œ ì„ íƒí•˜ì‹­ì‹œì˜¤.
ë°˜ë“œì‹œ ë‹¤ìŒ ê·œì¹™ì„ ì§€í‚¤ì„¸ìš”:
- ì•„ë˜ 8ê°œ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë§Œ ê³¨ë¼ì„œ, ì¹´í…Œê³ ë¦¬ëª…ë§Œ ì •í™•í•˜ê²Œ ì¶œë ¥í•˜ì„¸ìš”.
- ê° ì¹´í…Œê³ ë¦¬ëª… ì˜†ì˜ ì˜ˆì‹œ í‚¤ì›Œë“œë¥¼ ì°¸ê³ í•´ ê·¼ì ‘í•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”.
- ë‹¨, ì˜ë¯¸ê°€ ëª…í™•í•˜ì§€ ì•Šê±°ë‚˜, ë‚´ìš©ì´ ì—†ëŠ” ë‹¨ìˆœ ìš”ì²­Â·ìš•ì„¤Â·ë¶ˆí‰ì€ 'ê¸°íƒ€'ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
- ì˜¤ì§ ì¹´í…Œê³ ë¦¬ëª…ë§Œ ì¶œë ¥í•˜ì„¸ìš”. (ì„¤ëª…, ë²ˆí˜¸ ë“± ê¸ˆì§€)

{categories}

"""
    response = client.chat.completions.create(
        model="gpt-4.1",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=20,
        temperature=0
    )
    return response.choices[0].message.content.strip()

#ë¶„ë¥˜ ì ìš©
for idx, row in df_filtered.iterrows():
    review_text = row['ë¦¬ë·° í…ìŠ¤íŠ¸'][:100]
    try:
        category = classify_with_gpt(review_text)
    except Exception as e:
        print(f"ì˜ˆì™¸ ë°œìƒ: {e}")
        category = "ê¸°íƒ€"
    df_filtered.at[idx, 'ì¹´í…Œê³ ë¦¬'] = category
    time.sleep(1.2)


#ì›ë³¸ df_allì— ê²°ê³¼ ë°˜ì˜
df_all.loc[df_filtered.index, 'ì¹´í…Œê³ ë¦¬'] = df_filtered['ì¹´í…Œê³ ë¦¬']

summary = []

for company in ['ìì‚¬', 'ê²½ìŸì‚¬']:
    df_sub = df_all[df_all['êµ¬ë¶„'] == company]
    review_count = len(df_sub)
    positive = (df_sub['ë°˜ì‘'] == 'ê¸ì •').sum()
    negative = (df_sub['ë°˜ì‘'] == 'ë¶€ì •').sum()
    positive_ratio = round(negative / review_count * 100, 1) if review_count > 0 else 0
    avg_score = round(df_sub['í‰ì '].mean(), 2) if review_count > 0 else 0
    summary.append([company, review_count, positive, negative, positive_ratio, avg_score])

# 2. ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
summary_df = pd.DataFrame(summary, columns=['êµ¬ë¶„', 'ë¦¬ë·°ìˆ˜', 'ê¸ì •', 'ë¶€ì •', 'ë¶€ì •ë¹„ìœ¨', 'í‰ê· í‰ì '])

# [1] ë°ì´í„° ì €ì¥
df_all.to_csv('df_all.csv', index=False, encoding='utf-8-sig')
csv_file_path = 'df_all.csv'

# [2] ìì‚¬/ê²½ìŸì‚¬ë³„ ë¦¬ë·°Â·í‰ì  ìš”ì•½í‘œ
summary = []
for company in ['ìì‚¬', 'ê²½ìŸì‚¬']:
    df_sub = df_all[df_all['êµ¬ë¶„'] == company]
    review_count = len(df_sub)
    positive = (df_sub['ë°˜ì‘'] == 'ê¸ì •').sum()
    negative = (df_sub['ë°˜ì‘'] == 'ë¶€ì •').sum()
    negative_ratio = round(negative / review_count * 100, 1) if review_count > 0 else 0
    avg_score = round(df_sub['í‰ì '].mean(), 2) if review_count > 0 else 0
    summary.append([company, review_count, positive, negative, negative_ratio, avg_score])

summary_df = pd.DataFrame(summary, columns=['êµ¬ë¶„', 'ë¦¬ë·°ìˆ˜', 'ê¸ì •', 'ë¶€ì •', 'ë¶€ì •ë¹„ìœ¨', 'í‰ê· í‰ì '])

# [3] ì¹´í…Œê³ ë¦¬ë³„ ë¶€ì • ë¦¬ë·° ë¶„í¬í‘œ
df_neg = df_all[df_all['ë°˜ì‘'] == 'ë¶€ì •']
category_table = df_neg.pivot_table(
    index='ì¹´í…Œê³ ë¦¬',
    columns='êµ¬ë¶„',
    values='ë¦¬ë·° í…ìŠ¤íŠ¸',
    aggfunc='count',
    fill_value=0
).reset_index()
category_table.columns.name = None
total_ja = category_table['ìì‚¬'].sum()
total_comp = category_table['ê²½ìŸì‚¬'].sum()
category_table['ë¹„ì¤‘(ìì‚¬)'] = (category_table['ìì‚¬'] / total_ja * 100).round(2)
category_table['ë¹„ì¤‘(ê²½ìŸì‚¬)'] = (category_table['ê²½ìŸì‚¬'] / total_comp * 100).round(2)
category_table = category_table[['ì¹´í…Œê³ ë¦¬', 'ìì‚¬', 'ë¹„ì¤‘(ìì‚¬)', 'ê²½ìŸì‚¬', 'ë¹„ì¤‘(ê²½ìŸì‚¬)']]
category_order = [
    "ì•± ì ‘ì† ì˜¤ë¥˜", "ì¸ì‹/ë“±ë¡/íƒœê¹…/ë°œê¸‰", "ì¶©ì „/ì”ì•¡/ê²°ì œ/í™˜ë¶ˆ",
    "ê¸°ê¸° í˜¸í™˜/ì—…ë°ì´íŠ¸", "ê³ ê°ì„¼í„°/ë¬¸ì˜/CS",
    "UI/UX/ì´ìš©ë¶ˆí¸/ì¸ì¦", "ì•Œë¦¼/í‘¸ì‹œ/ê³µì§€", "ê¸°íƒ€"
]
category_table['ì¹´í…Œê³ ë¦¬'] = pd.Categorical(category_table['ì¹´í…Œê³ ë¦¬'], categories=category_order, ordered=True)
category_table = category_table.sort_values('ì¹´í…Œê³ ë¦¬')

# [4] HTML ë³€í™˜ (ì»¨í”Œë£¨ì–¸ìŠ¤ ë³¸ë¬¸)
summary_html = summary_df.to_html(index=False, border=1)
category_html = category_table.to_html(index=False, border=1)

# [5] ì£¼ìš” ì¸ì‚¬ì´íŠ¸(gpt_anal) ìƒì„±: ìµœê·¼ 20ê°œ ë¦¬ë·°ë§Œ ìš”ì•½ ì˜ˆì‹œ (ì‹¤ë¬´ì—ì„œëŠ” ë” ë§ì€ rowë¡œ í™•ì¥ ê°€ëŠ¥)
ja_texts = "\n".join(
    df_filtered[df_filtered['êµ¬ë¶„'] == 'ìì‚¬'].sort_values('ì‘ì„±ì¼', ascending=False)['ë¦¬ë·° í…ìŠ¤íŠ¸']
)
comp_texts = "\n".join(
    df_filtered[df_filtered['êµ¬ë¶„'] == 'ê²½ìŸì‚¬'].sort_values('ì‘ì„±ì¼', ascending=False)['ë¦¬ë·° í…ìŠ¤íŠ¸']
)


gpt_prompt = f"""ì•„ë˜ëŠ” ì•± ë¶€ì • ë¦¬ë·° ë°ì´í„° ì…ë‹ˆë‹¤.

[ìì‚¬ ë¶€ì • ë¦¬ë·°]
{ja_texts}

[ê²½ìŸì‚¬ ë¶€ì • ë¦¬ë·°]
{comp_texts}

ë¦¬ë·° ë°ì´í„°ë¥¼ ì°¸ê³ í•´ ì•„ë˜ ì–‘ì‹ì— ë§ê²Œ ìš”ì•½í•´ ì£¼ì„¸ìš”.

<h3>[ìì‚¬ ë¶€ì • ì´ìŠˆ TOP3]</h3>
<ul>
<li>ì‘ì„±í•´ì£¼ì„¸ìš”</li>
<li>ì‘ì„±í•´ì£¼ì„¸ìš”</li>
<li>ì‘ì„±í•´ì£¼ì„¸ìš”</li>
</ul>

<h3>[ê²½ìŸì‚¬ ë¶€ì • ì´ìŠˆ TOP3]</h3>
<ul>
<li>ì‘ì„±í•´ì£¼ì„¸ìš”</li>
<li>ì‘ì„±í•´ì£¼ì„¸ìš”</li>
<li>ì‘ì„±í•´ì£¼ì„¸ìš”</li>
</ul>

<h3>[ì „ì²´ ì¸ì‚¬ì´íŠ¸/íŠ¸ë Œë“œ]</h3>
<ul>
<li>ì‘ì„±í•´ì£¼ì„¸ìš”</li>
<li>ì‘ì„±í•´ì£¼ì„¸ìš”</li>
<li>ì‘ì„±í•´ì£¼ì„¸ìš”</li>
</ul>

ê³µí†µê·œì¹™:
- let's think step by step and work through this carefully
- ë¶€ì • ì´ìŠˆ TOP3 : "ìì‚¬ ë¶€ì • ë¦¬ë·°", "ê²½ìŸì‚¬ ë¶€ì • ë¦¬ë·°" ê° ë¦¬ë·°ë¥¼ ì½ê³  ìì‚¬,ê²½ìŸì‚¬ë³„ ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ ìˆœì„œë¡œ top3ë¥¼ ì•Œë ¤ì¤ë‹ˆë‹¤. ì´ëŠ” ì–´ë–¤ ì´ìŠˆê°€ ê°€ì¥ ë§ì€ì§€ í™•ì¸í•˜ê¸° ìœ„í•¨ìœ¼ë¡œ ì •í™•í•´ì•¼ í•©ë‹ˆë‹¤.
- ë¶€ì • ì´ìŠˆëŠ” êµ¬ì²´ì ì¸ ë¬¸ì œ ìœ í˜•(ì˜ˆ: ì¶©ì „ ì˜¤ë¥˜, ì¹´ë“œ ë“±ë¡ ì‹¤íŒ¨, ì•± íŠ•ê¹€ ë“±) ìœ„ì£¼ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.
- ì „ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸/íŠ¸ë Œë“œ: ë°˜ë³µì ìœ¼ë¡œ ì–¸ê¸‰ë˜ëŠ” ì´ìŠˆ, íŠ¹ì • ì„œë¹„ìŠ¤/ê¸°ëŠ¥ì— ì§‘ì¤‘ëœ ë¶ˆë§Œ ë“± ë°ì´í„°ì—ì„œ í™•ì¸ë˜ëŠ” íŠ¹ì§•ì„ 3ì¤„ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.
- ê²°ê³¼ë¬¼ì€ ì»¨í”Œë£¨ì–¸ìŠ¤ APIì— body_htmlë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.
- ì•„ë˜ html ì–‘ì‹ì—ì„œ <li>ì‘ì„±í•´ì£¼ì„¸ìš”</li> ë¶€ë¶„ë§Œ ì‹¤ì œ ë¶„ì„ ê²°ê³¼ë¡œ ì±„ì›Œì£¼ê³ , ë‚˜ë¨¸ì§€ html íƒœê·¸ì™€ êµ¬ì¡°(ì œëª©, ì¤„ë°”ê¿ˆ ë“±)ëŠ” ì ˆëŒ€ ìˆ˜ì •í•˜ê±°ë‚˜ ë³€í˜•í•˜ì§€ ë§ˆì„¸ìš”.
- ë°˜ë“œì‹œ ì§€ì •í•´ì¤€ html ì–‘ì‹ì„ í•œ ê¸€ìë„ ë¹ ì§ì—†ì´ ê·¸ëŒ€ë¡œ ìœ ì§€í•´ì£¼ì„¸ìš”.
"""

# GPT API í˜¸ì¶œ
response = client.chat.completions.create(
    model="gpt-4.1",
    messages=[
        {"role": "system", "content": "ë„ˆëŠ” ë°ì´í„° ë¶„ì„ê°€ì•¼."},
        {"role": "user", "content": gpt_prompt}
    ],
    max_tokens=512,
    temperature=0.5,
)
gpt_anal_text = response.choices[0].message.content.strip()
gpt_anal = f"<pre>{gpt_anal_text}</pre>"

# [6] body_htmlì— gpt_anal ì‚½ì…
body_html = f"""
<h2>ğŸ“Š 1. ë¦¬ë·°/í‰ì  ìš”ì•½í‘œ</h2>
{summary_html}
<br>
<h2>ğŸ“ 2. ì¹´í…Œê³ ë¦¬ë³„ ë¶€ì • ë¦¬ë·° ë¶„í¬</h2>
{category_html}
<br>
<h2>ğŸ’¡ 3. ì£¼ìš” ì¸ì‚¬ì´íŠ¸</h2>
{gpt_anal}
<br>
<h2>ğŸ“ [Raw Data] ì „ì²´ ë¦¬ë·° ë°ì´í„° ë‹¤ìš´ë¡œë“œ</h2>
"""

# [7] Confluence í˜ì´ì§€ ìƒì„±
confluence_domain = 'myezl.atlassian.net'


headers = {
    'Authorization': 'Basic ' + base64.b64encode(f'{confluence_api_user}:{confluence_api_token}'.encode()).decode(),
    'Content-Type': 'application/json'
}
base_url = f'https://{confluence_domain}/wiki/rest/api/content/'

page_data = {
    "type": "page",
    "title": title,
    "ancestors": [{"id": parent_page_id}],
    "space": {"key": space_key},
    "body": {
        "storage": {
            "value": body_html,
            "representation": "storage"
        }
    }
}
response = requests.post(base_url, headers=headers, json=page_data)
assert response.status_code in [200, 201], "í˜ì´ì§€ ìƒì„± ì‹¤íŒ¨: " + response.text
new_page_id = response.json()['id']
print("í˜ì´ì§€ ìƒì„± ì™„ë£Œ:", new_page_id)

# [8] ì²¨ë¶€íŒŒì¼ ì—…ë¡œë“œ (sleep 2ì´ˆ í›„)
time.sleep(2)
attach_url = f'https://{confluence_domain}/wiki/rest/api/content/{new_page_id}/child/attachment'
attach_headers = {
    'Authorization': 'Basic ' + base64.b64encode(f'{confluence_api_user}:{confluence_api_token}'.encode()).decode(),
    'X-Atlassian-Token': 'no-check'
}
with open(csv_file_path, 'rb') as f:
    files = {'file': (os.path.basename(csv_file_path), f, 'text/csv')}
    attach_response = requests.post(attach_url, headers=attach_headers, files=files)

print("API ì‘ë‹µ:", attach_response.status_code)
print(attach_response.text)

assert attach_response.status_code in [200, 201], "ì²¨ë¶€íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: " + attach_response.text
print("ì²¨ë¶€íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ")

# [9] ë³¸ë¬¸ì— ë‹¤ìš´ë¡œë“œ ë§í¬ PATCH
download_url = f"https://{confluence_domain}/wiki/download/attachments/{new_page_id}/df_all.csv"
patch_url = f'https://{confluence_domain}/wiki/rest/api/content/{new_page_id}'
patch_headers = headers
patch_data = {
    "version": {"number": 2},
    "title": title,
    "type": "page",
    "body": {
        "storage": {
            "value": body_html + f'<br><a href="{download_url}">df_all.csv ë‹¤ìš´ë¡œë“œ</a><br>',
            "representation": "storage"
        }
    }
}
patch_response = requests.put(patch_url, headers=patch_headers, json=patch_data)
print("ë³¸ë¬¸ì— ë‹¤ìš´ë¡œë“œ ë§í¬ ì¶”ê°€ ì™„ë£Œ:", patch_response.status_code)
